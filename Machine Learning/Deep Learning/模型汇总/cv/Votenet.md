# 论文
[vote](https://arxiv.org/pdf/1904.09664)
# 简介
**背景：** 
3d点云检测面临的挑战：3D的物体的中心（质心）是远离物体的表面的，而点云中的点都是扫描物体表面得到的，这样点云的内部就是空心的，因此就很难去找到物体中心。

**作者提出的方法：**
为了应对这一挑战，作者提出了 VoteNet，这是一种基于深度点集网络和***霍夫投票***协同作用的端到端 3D 对象检测网络。我们的模型通过简单的设计、紧凑的模型尺寸和高效率，在两个大型真实 3D 扫描数据集 ScanNet 和 SUN RGB-D 上实现了最先进的 3D 检测。值得注意的是，VoteNet 通过使用纯粹的几何信息而不依赖彩色图像，优于以前的方法。通过直接处理点云数据，不仅避免了信息的损失，而且由于处理的点云数据的稀疏性，还可以降低计算代价。

# 核心思路
使用深度 Hough 投票模型进行点云中的 3D 对象检测。给定 3D 场景的点云，我们的 VoteNet 投票给对象中心，然后对投票进行分组和聚合，以预测 3D 边界框和对象的语义类别

![](assets/Votenet/Votenet1.png)

# 模型框架
![](assets/Votenet/Votenet2.png)



***核心框架：***$$\text{PointNet++ (Backbone)} \rightarrow \text{Voting Module} \rightarrow \text{Vote Aggregation} \rightarrow \text{Detection Branch (Predict BBox)}$$
1. 网络结构由**原始点云输入**，维度为(20000,3)，代表输入20000个点云，每个点云都有xyz三个坐标
2. 作者使用**pointnet++骨干网络提取特征**，骨干网络的输入为原始点云，维度为(20000,3)，pointnet++主要通过4层SA模块和2层FP模块进行特征提取，经过骨干网络提取出来1024个点，这些点称为种子点(或兴趣点)，种子点坐标维度为(8,1024,3)，种子点的特征维度为(8,1024,256)，8是batch，1024是种子点的个数，3是xyz坐标，256是特征数
3. 将提取出来的种子点送到**votelayer进行投票**，这里种子点特征将通过三层一维卷积和两层BN层，输出一个张量offset，维度为(8,1024,1,259)，votelayer输出的是种子点到投票点的位移偏移量offset以及投票点的特征vote_features，所以要的到投票点的坐标需要用种子点的坐标加上得到的偏移量，vote_xyz=seed_xyz+offset，投票点坐标的维度也是(8,1024,3),投票点的特征是将输出特征加上种子点特征得到的vote_features=seed_features+offset，(由于输出的维度和种子点维度不一致，这里还需要将维度进行变换才可以相加)
4. 得到投票点坐标以及投票点特征后需要对投票点进行**聚合**，作者使用的是pointnet中的SA模块进行聚合，将1024个投票点聚合成256个点aggregated_points，维度为(8,256,3），其特征为aggregated_features(8,256,128）
5. 将聚合好的投票点进行**候选框预测**， 每个聚合后的点会投票产生一个候选结果。根据聚合后的特征，VoteNet通过分类head和回归head产生预测的候选框结果。aggregated_features（256x128）通过两层一维卷积进行更深层特征提取，提取后的特征分别经过一层卷积得到分类预测结果cls_precls_predictions（256x12）和位置回归预测结果reg_predictions（256x67）
6. 最后一步就是将结果进行**解码**，预测结果如何与真实标签关联需要一一进行解码。真实标签主要包含目标有无、类别标签和目标回归位置。



# Vote机制

## 1.投票模块 (Voting Module): 将点云中的点偏移到物体中心，产生了密集的投票簇。
**霍夫变换 (Hough Transform)** ，它是一种强大的技术，常用于图像处理中检测形状，比如直线或圆。它的核心思路是：将空间中的一个数据点（比如图像中的一个边缘点）映射到参数空间（称为累加器空间），让这个点**投票**给所有可能的形状参数。形状参数的交汇处（票数最多的地方）就是检测到的形状。

VoteNet 正是将这个思想搬到了 3D 点云的深度学习中：

- **输入点与投票:** VoteNet 的骨干网络（例如 PointNet++）从输入点云中提取特征，然后将特征输入到**投票模块 (Voting Module)** 中。
    
- **预测偏移:** 投票模块让点 $\mathbf{x}_i$ 预测一个 3D 偏移向量 $\mathbf{v}_i$（即 $\Delta x, \Delta y, \Delta z$），这个向量指向它所属对象的**中心**。
    
- **中心聚集:** 最终，多个点投出的票会聚集在真实对象中心的位置，形成高密度的**投票簇 (Vote Clusters)**。
    
这种方法巧妙地利用了深度学习的能力来学习精确的偏移量，实现了“深度”投票。


## 2.投票聚合 (Set Abstraction): 在这些投票点上应用 PointNet++ 的 Set Abstraction (FPS + 球查询 + PointNet)，实现了高效的特征提取和降维。

# 检测分支 (Detection Branch)

VoteNet 的 **检测分支** 主要负责从聚合的特征向量中预测以下四个关键信息：

- **1. 类别标签 (Class Label) 🏷️:** 这个物体是什么（车、人、自行车等）。
    
- **2. 尺寸 (Size/Box size) 📏:** 它的长 $L$、宽 $W$、高 $H$。
    
- **3. 方向角 (Heading) 🧭:** 它的旋转角度 $\theta$。
    
- **4. 置信度 (Confidence Score) 💯:** 这个预测的可靠性。


# Votenet 模拟数据流转过程
这是一个非常棒的进阶问题。**VoteNet** 是在 PointNet++ 的基础上发展起来的，专门用于 **3D 目标检测**。

理解 VoteNet 的核心在于理解它的名字——**“投票 (Vote)”**。在 PointNet++ 中，点仍然在物体的**表面**；而在 VoteNet 中，数据会发生一次“位移”，从物体表面跳到物体的**中心**。

我们将沿用上面的模拟数据风格，假设场景里有一个 **“桌子”**。

---

### 0. 初始场景设定 (The Context)

假设经过 PointNet++ 的提取（Backbone），我们得到了一组 **种子点 (Seed Points)**。
为了简化，我们要检测一个长方体桌子。
*   **输入**：4个种子点（来自 PointNet++ 的输出）。
*   **物理位置**：
    *   $S_1, S_2$：位于桌子的两条腿上（表面）。
    *   $S_3$：位于桌面的边缘（表面）。
    *   $S_4$：位于远处的墙面上（噪音/背景）。
*   **桌子的真实中心**：$(5, 5, 0)$。

---

### 第一阶段：霍夫投票 (Hough Voting) —— 从表面到中心
**目标**：让每个点“猜”物体的中心在哪里。这是 VoteNet 的灵魂。
**输入**：种子点坐标 $(4, 3)$ + 种子点特征 $(4, 16)$ (假设特征维是16)。

#### 1. 生成偏移量 (Vote Generation MLP)
网络通过一个 MLP（共享权重）处理每个种子点的特征，输出一个坐标偏移量 $(\Delta x, \Delta y, \Delta z)$ 和特征残差。

*   **模拟计算**：
    *   $S_1 (4, 4, 0)$ \[桌腿]：它“感觉”自己在桌子左下角，所以预测中心在右上方。
        *   输出偏移：$(+1, +1, 0)$
    *   $S_2 (6, 6, 0)$ \[另一条腿]：它“感觉”自己在桌子右上方，所以预测中心在左下方。
        *   输出偏移：$(-1, -1, 0)$
    *   $S_3 (5, 4, 0)$ \[桌面边缘]：
        *   输出偏移：$(0, +1, 0)$
    *   $S_4 (0, 0, 0)$ \[墙面]：它不知道自己在哪里，乱猜或输出微小的偏移。
        *   输出偏移：$(0.1, 0.2, 0)$

#### 2. 执行投票 (Applying Votes)
**公式**：$Vote = Seed + Offset$

*   **数据变化 (Vote Coordinates)**：
    $$
    \begin{bmatrix}
    V_1: (4+1, 4+1, 0) \to (5, 5, 0) \quad \text{// 完美命中中心} \\
    V_2: (6-1, 6-1, 0) \to (5, 5, 0) \quad \text{// 完美命中中心} \\
    V_3: (5+0, 4+1, 0) \to (5, 5, 0) \quad \text{// 完美命中中心} \\
    V_4: (0.1, 0.2, 0) \quad \text{// 还在墙角附近}
    \end{bmatrix}
    $$
*   **特征变化**：种子特征 + 特征残差 $\to$ 投票特征 $(4, 16)$。
    *(注：现在 $V_1, V_2, V_3$ 虽然来源不同，但在空间坐标上重叠了！这就是“聚类”的基础)*

---

### 第二阶段：投票聚类 (Vote Clustering)
**目标**：在一堆杂乱的投票中，找到票数最集中的地方（即物体中心）。
**过程**：采样 (FPS) -> 分组 (Ball Query) -> 聚合 (PointNet)

#### 1. 采样与分组 (FPS + Grouping)
我们在 **Vote 空间** 进行最远点采样，假设选 **2个** 聚类中心。

*   **采样**：
    1.  选中 $V_1 (5,5,0)$（因为它代表了 $V_1, V_2, V_3$ 这一堆重叠的点）。
    2.  选中 $V_4 (0.1, 0.2, 0)$（因为它离 $V_1$ 很远）。

*   **成组 (Ball Query)**：
    设定半径 $R=2.0$。
    *   **Cluster A (Center $V_1$)**：
        *   $V_1, V_2, V_3$ 都在半径内（距离为0）。
        *   索引：$[0, 1, 2]$
        *   这是一组高质量的投票，都指向桌子。
    *   **Cluster B (Center $V_4$)**：
        *   只有 $V_4$ 自己。
        *   索引：$[3]$
        *   这是背景噪音。

#### 2. 特征聚合 (Vote Aggregation)
对每个 Cluster 里的特征进行 PointNet 处理（MLP + Max Pooling）。
*   **Cluster A**：融合 $V_1, V_2, V_3$ 的特征。因为它们来自桌子的不同部位，特征互补，融合后能描述整个桌子。
*   **Cluster B**：只有 $V_4$ 的特征，特征很弱。

*   **输出数据**：
    *   2个 聚合后的特征向量，假设维度 $(2, 128)$。
    *   2个 物理中心坐标 $(V_1, V_4)$。

---

### 第三阶段：候选框生成 (Proposal Generation)
**目标**：根据聚合后的特征，画出 3D 框，并判断它是什​​么。
**过程**：MLP 分支预测

我们将 Cluster A 的特征向量输入 MLP，产生以下预测（Cluster B同理，但分数会很低）：

1.  **Objectness Score (是物体的概率)**
    *   Cluster A: `0.95` (非常有信心)
    *   Cluster B: `0.05` (认为是背景)

2.  **Center Refinement (中心微调)**
    *   虽然 $V_1$ 已经在 $(5,5,0)$，但模型可能觉得还需要微调一下。
    *   $\Delta_{refine} = (0.01, -0.01, 0)$
    *   最终中心：$(5.01, 4.99, 0)$

3.  **Heading (朝向角度)**
    *   输出一个角度值，比如 $0^{\circ}$ (桌子摆得很正)。

4.  **Size (尺寸)**
    *   输出长宽高 $(L, W, H)$。
    *   预测值：$(2.0, 1.0, 0.8)$ (米)。

5.  **Semantic Class (语义类别)**
    *   输出类别概率向量。
    *   `[Table: 0.9, Chair: 0.1, Sofa: 0.0]`

---

### 总结数据流转图 (与 PointNet++ 对比)

| 步骤 | 模块 | 关键动作 | 数据坐标物理含义 | 模拟数据状态 |
| :--- | :--- | :--- | :--- | :--- |
| 1 | **Backbone** | Feature Extraction | **物体表面** | $P_1, P_2$ 在桌腿上 |
| 2 | **Voting** | **Offset Generation** | **从表面飞向中心** | $P_1$ 加偏移量变成 $V_1$ |
| 3 | | **Apply Vote** | **虚拟中心点** | $V_1, V_2, V_3$ 全部聚在 $(5,5,0)$ |
| 4 | **Clustering**| **FPS + Grouping** | **潜在物体中心** | 发现 $V_1, V_2, V_3$ 是一伙的 (Cluster A) |
| 5 | **Proposal** | **MLP Prediction** | **3D 边界框** | Cluster A $\to$ "这是一张桌子，尺寸2x1" |

### 核心区别点
PointNet++ 的 SA/FP 过程一直在处理**真实存在的点**（LiDAR打到的点）。
VoteNet 的数据流中，产生了一组**虚拟点（Votes）**。
*   数据流转的中间，坐标矩阵从 **"稀疏分布在物体表面"** 变成了 **"高密度聚集在物体内部中心"**。
*   这就是 VoteNet 处理遮挡和稀疏点云能力强的核心原因——只要桌子还有一条腿（有种子点），它就能投出桌子的中心，进而生成完整的框。